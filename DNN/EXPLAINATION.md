# 全连接网络(DNN)

2024/5/20

## 全连接网络的缺点

- DNN的超参数过多，不好调参与构建模型。
- DNN的无关权重过多，浪费资源。
- DNN需要较多已经标签好的数据。
- DNN自身具有很强的黑盒机制，解释性具有很大欠缺。

## 全连接网络的相关论文

链接：[https://pan.baidu.com/s/1uepryJ7kXFMgAigsmsgolA?pwd=4ui8](https://pan.baidu.com/s/1uepryJ7kXFMgAigsmsgolA?pwd=4ui8)
提取码：4ui8

## 全连接网络的算式(对于一个模型而言，一次训练就只有一次前向传播和一次反向传播)

## 前向传播算式:

$A=σ(W_{i,j} \cdot x_{i-1} +b_{i,j}）$

- 此方程为单个神经元的前向传播方程式，整层的前向传播的输出向量为每个神经元的前向传播标量结果组成一维向量$x_i$；
- $W_{i,j}$是第i层网络的第j个神经元的权重矩阵；$b_{i,j}$是第i层网络的第j个神经元的偏移值；
- $x_{i-1}$是上一层网络所有神经元的前向传播标量结果组成一维向量；
- σ()为激励函数，注意激励函数的输出为标量；(如果激活函数的输入是矩阵,会在输出前对激活函数的输出结果，用池化将输出矩阵中的信息聚合成一个标量)
- A为单个神经元的前向传播标量结果；
- 由介质公式：$z_{i,j}=W_{i,j} \cdot x_{i-1} + b_{i,j}$可知，$z_{i,j}$为单个神经元的加权输入；

## 反向传播算式：

### 输出层误差项算式：

$$
B = \frac{∂L(σ_i(z_{i,j},yTure_j)}{∂σ_i(z_{i,j})} \cdot \frac{∂σ_i(z_{i,j})}{∂z_{i,j}}
$$

- 此方程为输出层单个神经元的计算标量误差项B的方程，整层误差项是每个神经元的标量误差项组成一维向量$δ_i$；
- 方程中误差项B和$\frac{∂L(σ_i(z_{i,j},yTure_j)}{∂σ_i(z_{i,j})}$损失函数的激活函数的偏导数和$\frac{∂σ_i(z_{i,j})}{∂z_{i,j}}$激励函数的加权输入的偏导数都是标量；
- 函数L是整个模型的损失函数，其自变量是对应单个神经元的激活函数标量输出$σ_i(z_{i,j}）$和标签的实际值$yTure_j$;(注:输出层的$σ_i(z_{i,j}）=yPredicting_i$)
- $σ_i(z_{i,j}）$是对应单个神经元的激活函数标量输出;$z_{i,j}$是单个神经元的加权输入；

### 隐藏层误差项算式：

$$
C = W_{σ_i(z_{i,j})} \cdot δ_{i+1} \cdot \frac{∂σ_i(z_{i,j})}{∂z_{i,j}}
$$

- 此方程为隐藏层单个神经元的计算标量误差项C的方程，整层误差项是每个神经元的标量误差项组成一维向量$δ_i$；
- $W_{σ_i(z_{i,j})}$是对应单个神经元输出的标量激活值$σ_i(z_{i,j})$于下一层每个神经元的权重向量中使用过的不同标量权重，所组成的$σ_i(z_{i,j})$的权重向量；
- $δ_{i+1}$是下一层的误差项的向量；$W_{x_i} \cdot δ_{i+1}$为向量相乘为标量；

### 权重与偏移值的梯度算式：

$$
\frac{∂L}{∂b_i} = δ_i  \\
\frac{∂L}{∂W_i} = δ_i \cdot (x_{i-1})^T
$$

- 该两方程求整个一层网络的偏移值梯度向量$\frac{∂L}{∂b_i}$，权重梯度矩阵$\frac{∂L}{∂W_i}$；
- $δ_i$为对应层的误差项的向量；$x_{i-1}$为上一层的输出激活值的向量；
- $δ_i \cdot (x_{i-1})^T$的结果为矩阵；

### 更新权重和偏移值的算法：

$$
b_i = b_i -η\cdot \frac{∂L}{∂b_i} \\
W_i = W_i -η\cdot \frac{∂L}{∂W_i}
$$

- **η** 是学习率;(学习率是一个超参数，它需要由用户手动设置，决定了权重和偏移值更新的幅值)
- 该两方程的运算都是线性代数的减法，遵循同位置相减的原则；
## 残差机制(缓解梯度消失问题)
$$
y = F(x) + x
$$
- F(x)就是原本层的输出
  x是原始数据；

