# 对抗生成式网络(GAN)

2024/5/21

## GAN的本质

GAN由两个深度学习模型组成的训练框架：

一个生成模型G，以噪音向量为训练数据，以1的矩阵为目的标签训练生成虚假的样本；(生成器G的损失函数来自于判别器D)

一个判别模型D，用于估计样本是否来自训练数据而非生成模型G；

这两个模型通过对抗过程进行交替训练，其中G试图欺骗D，而D则努力区分真实数据和生成数据。（其本质是判别器D与生成器G对于同一损失函数的追求相反;）

## GAN的相关论文

链接：[https://pan.baidu.com/s/1TbaaCcV4iUtt1HmHn9_Qyw?pwd=tsq8](https://pan.baidu.com/s/1TbaaCcV4iUtt1HmHn9_Qyw?pwd=tsq8)
提取码：tsq8

## GAN的训练流程(GAN与两个独立交替训练的深度学习模型的唯一区别损失函数)

![GAN的示例](../assets/GAN的示例.png)

### 一次训练(迭代)中的判别器训练的要点：（一次训练中先训练一次判别器，再训练一次生成器）

- 判别器训练时，固定生成器的权重和偏移值，活化判别器的权重和偏移值；
- 用随机噪音作为输入数据，1的矩阵为目标标签给生成器生成伪造数据；
- 将生成器的输出伪造数据(目标标签为0的矩阵)和真实数据(目标标签为1的矩阵)分别作为判别器的训练数据得到，真实数据被判别为真实数据的概率**D**(**x**)，伪造数据被判别为真实数据的概率**D**(**G**(**z**))；
- 判别器的损失函数：$L_D=-(E_{x \sim p_data(x)}[\log{D(x)}] + E_{z \sim p_data(z)}[\log{(1-D(G(z)))}])$;
  - D(**x**)是真实数据被判别为真实数据的概率；D(G(z))是伪造数据被判别为真实数据的概率；
  - $E_{x \sim p_data(x)}[\log{D(x)]}$是样本集为x的D(x)的对数期望，且x是来自于真实数据集$p_data(x)$;
  - $E_{z \sim p_data(z)}[\log{D(G(z))]}$是样本集为z的D(G(z))函数的对数期望，且z是来自于随机生成的噪音数据集$p_z$;
  - **G**(**z**)代表生成器的输出伪造数据；

### 一次训练(迭代)中的生成器训练的要点：（一次训练中先训练一次判别器，再训练一次生成器）

- 生成器训练时，固定判别器的权重和偏移值，活化生成器的权重和偏移值；
- 用随机噪音作为输入数据，1的矩阵为目标标签给生成器生成伪造数据；
- 将生成器的输出伪造数据(目标标签为0的矩阵)单独作为判别器的训练数据得到，伪造数据被判别为真实数据的概率**D**(**G**(**z**))；
- 生成器的损失函数：$L_G=- E_{z \sim p_data(z)}[\log{D(G(z))]}$;

  - D(G(z))是伪造数据被判别为真实数据的概率；
  - $E_{z \sim p_data(z)}[\log{D(G(z))]}$是样本集为z的D(G(z))函数的对数期望，且z是来自于随机生成的噪音数据集$p_z$;
  - **G**(**z**)代表生成器的输出伪造数据；

### 注意：

$L_G$与$L_D$是负相关，且都来自于binary\_crossentropy的负数（二分类交叉熵的负数，二分类交叉熵本身可以作为损失函数）:

$min_G max_D V(D,G)=E_{x \sim p_data(x)}[\log{D(x)]} + E_{z \sim p_data(z)}[\log{(1-D(G(z))}]$;

$L_G$与$L_D$的区别是,判别器训练时，x在x的位置，z在z的位置；生成器训练时，无x，z就在原本x的位置了；

即gan和discriminator的损失函数都用二分类交叉熵；generator不设置损失函数，因为generator的损失函数的计算来自于整个gan的discriminator部分；
