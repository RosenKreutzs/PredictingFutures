{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "from keras import Input\n",
    "from keras.datasets import boston_housing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "\n",
    "def drop_split_future(data,drop_future,target):#删除无关特征，提取标签\n",
    "    target_future=data[target]\n",
    "    # 删除指定的列特征\n",
    "    data = data.drop(drop_future, axis=1)\n",
    "    data = data.round(2)\n",
    "    return data,target_future\n",
    "\n",
    "\n",
    "df=pd.read_csv(\"../data_files/ADS/futures_data3.csv\")\n",
    "X1,y1=drop_split_future(df,[\"commodity\",\"date\",\"tomorrow_close\"],\"tomorrow_close\")\n",
    "X1 = X1.drop(X1.index[0])\n",
    "X0=X1.to_numpy()\n",
    "y1 = y1.drop(X1.index[0])\n",
    "y0=y1.to_numpy()\n",
    "# 将数据集分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "df1 = df.drop([\"commodity\",\"date\"], axis=1)\n",
    "df1 = df1.round(2)\n",
    "df_numpy=df1.to_numpy()\n",
    "\n",
    "   "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 将GAN应用于回归问题的方法\n",
    "从原理和应用场景上来看，GAN模型并不直接适用于回归问题。\n",
    "但我将生成器generator的生成对象设为预测特征tomorrow_close，\n",
    "再将判别器discriminator的输入对象设为生成器的生成对象与其余18个特征的拼接向量，\n",
    "就能实现GAN模型对于回归问题的预测了。"
   ],
   "id": "aa9462a4ba9c336a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    " # 构建生成器\n",
    "generator_input = Input(shape=(17,))\n",
    "x = Dense(128)(generator_input)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "generator_output = Dense(1, activation='tanh')(x)\n",
    "generator = Model(generator_input, generator_output)\n",
    "\n",
    "    # 编译生成器\n",
    "generator.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "    # 构建判别器\n",
    "discriminator_input = Input(shape=(18,))  # 假设你想拼接后的输入是19维\n",
    "x = Dense(128)(discriminator_input)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "discriminator_output = Dense(1, activation='sigmoid')(x)\n",
    "discriminator = Model(discriminator_input, discriminator_output)\n",
    "\n",
    "    # 编译判别器\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # 构建 GAN 模型\n",
    "discriminator.trainable = False\n",
    "generator.trainable = True\n",
    "gan_input = generator_input  # GAN模型的输入与生成器的输入相同\n",
    "gan_output = discriminator(Concatenate()([generator_output, generator_input]))\n",
    "gan = Model(gan_input, gan_output)\n",
    "\n",
    "    # 编译GAN模型\n",
    "gan_optimizer = Adam()\n",
    "gan.compile(loss='binary_crossentropy', optimizer=gan_optimizer)\n",
    "\n",
    "# 训练 GAN 模型\n",
    "epochs = 1000\n",
    "batch_size = 128\n",
    "sample_interval = 100\n",
    "\n",
    "    # 初始化一些变量来保存训练过程中的损失\n",
    "d_loss_real = []\n",
    "d_loss_fake = []\n",
    "g_loss = []\n",
    "print(df_numpy)\n",
    "for epoch in range(epochs):\n",
    "        # ---------------------\n",
    "        #  训练判别器\n",
    "        # ---------------------\n",
    "    discriminator.trainable = True\n",
    "    generator.trainable = False\n",
    "    # 选择真实样本和噪声样本\n",
    "    idx = np.random.randint(0, df_numpy.shape[0], batch_size)\n",
    "    imgs_real = df_numpy[idx]  # 使用iloc基于整数位置索引行\n",
    "    imgs_real.astype(np.float32)\n",
    "    noise = np.random.normal(0, 1, (batch_size, 17))\n",
    "    # 生成“假”样本\n",
    "    imgs_fake0 = generator.predict(noise)\n",
    "    imgs_fake = np.concatenate((noise, imgs_fake0), axis=1)\n",
    "    # 训练判别器识别真实样本\n",
    "\n",
    "    d_loss_real_curr = discriminator.train_on_batch(imgs_real, np.ones((batch_size, 1)))#inputing shape (None, 19)\n",
    "        # 训练判别器识别“假”样本\n",
    "\n",
    "    d_loss_fake_curr = discriminator.train_on_batch(imgs_fake, np.zeros((batch_size, 1)))\n",
    "        # 计算判别器的总损失\n",
    "    d_loss = 0.5 * np.add(d_loss_real_curr, d_loss_fake_curr)\n",
    "\n",
    "        # ---------------------\n",
    "        #  训练生成器\n",
    "        # ---------------------\n",
    "    discriminator.trainable = False\n",
    "    generator.trainable = True\n",
    "    # 生成器希望判别器认为其输出是真实的\n",
    "    idx0 = np.random.randint(0, X0.shape[0], batch_size)\n",
    "    X0_real = X0[idx0]  # 使用iloc基于整数位置索引行\n",
    "    X0_real.astype(np.float32)\n",
    "    y0_real=y0[idx0]\n",
    "    y0_real.astype(np.float32)\n",
    "        # 训练生成器\n",
    "    g_loss_curr = generator.train_on_batch(X0_real, y0_real)\n",
    "        # 记录损失\n",
    "    d_loss_real.append(d_loss_real_curr)\n",
    "    d_loss_fake.append(d_loss_fake_curr)\n",
    "    g_loss.append(g_loss_curr)\n",
    "        # 打印和绘制进度条\n",
    "    print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss[0]))\n",
    "\n",
    "with open('futures_gan_model.pkl', 'wb') as file:\n",
    "    pickle.dump(generator, file)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "b55f06baee3d5161"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
